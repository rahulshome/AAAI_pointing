\section{Related work}
\label{related-work}
Task communication and interpretation is an active research area in robotics and AI. Traditionally in robotic manipulation, tasks are specified using parameterized transformations over modeled templates of objects. Recently \cite{manuelli2019kpam} proposed a way to model the costs and constraints in robotic manipulation tasks via visual key points on objects which can be grounded relatively easily from language and are effective in capturing the variance in geometries within object categories for task specification. While some other work model the robotâ€™s understanding of the task via a reward-based reinforcement learning \cite{gualtieri2018pick} or via imitation learning \cite{ghasemipour2019divergence}, it is unclear how such learned representations can be used for two-way communication. Thus there is a need for interpretable representations that can be effective for a two-way task communication between robots and humans.

Enabling robots to understand and generate instructions to collaboratively solve tasks with humans is an active area of research in natural language processing and human-robot interaction \cite{cha2018survey,butepage2017human}.   This work focuses on research that looks at the role of pointing gestures in communication.

Initial efforts in robotics have looked at making pointing gestures legible, adapting the process of motion planning so that robot movements are correctly understood as being directed toward the location of a particular object in space \cite{holladay2014legible,zhao2016experimental}.  The current work uses motions that are legible in this sense, and goes on to explore how precise the targeting has to be to signal an intended interpretation.

In natural language processing research, it's common to use an expanded pointing cone to describe the possible target objects for a pointing gesture, based on findings about human pointing \cite{kranstedt2003deixis,rieser2004pointing}.  In cluttered scenes, the pointing cone typically includes a region with many candidate referents.  Understanding and generating object references in these situations involves combining pointing with natural language descriptions \cite{han2018placing,kollar2014grounding}.  While we also find that many pointing gestures are ambiguous and can benefit from linguistic supplementation, our results challenge the assumption of a uniform pointing cone. We argue for an alternative, context-sensitive model.

In addition to gestures that identify objects, we also look at pointing gestures that identify points in space.  The closest related work involves navigation tasks, where pointing can be used to discriminate direction (e.g., left vs right) \cite{mei2016listen,tellex2011understanding}.  The spatial information needed for pick-and-place tasks is substantially more precise. Our findings suggest that this precision significantly impacts how pointing is interpreted and how it should be modeled.



