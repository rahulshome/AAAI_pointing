% Communication is anchored to the material world, to space, to events and people. One way it gets anchored is through pointing. 

Enabling robots to understand and generate instructions to collaboratively solve tasks with humans is an active area of research in natural language processing and human-robot interaction, see \cite{cha2018survey,butepage2017human} for review.   We focus here specifically on research that looks at the role of pointing gestures in communication.

Initial efforts in robotics have looked at making pointing gestures legible, adapting the process of motion planning so that robot movements are correctly understood as being directed toward the location of a particular object in space \cite{holladay2014legible,zhao2016experimental}.  Our work uses motions that are legible in this sense, and goes on to explore how precise the targeting has to be to signal an intended interpretation.

In natural language processing research, it's common to use an expanded pointing cone to describe the possible target objects for a pointing gesture, based on findings about human pointing \cite{kranstedt2003deixis,rieser2004pointing}.  In cluttered scenes, the pointing cone typically includes a region with many candidate referents.  Understanding and generating object references in these situations involves combining pointing with natural language descriptions \cite{han2018placing,kollar2014grounding}.  While we also find that many pointing gestures are ambiguous and can benefit from linguistic supplementation, our results challenge the assumption of a uniform pointing cone; we argue for an alternative, context-sensitive model.

In addition to gestures that identify objects, we also look at pointing gestures that identify points in space.  The closest related work involves navigation tasks, where pointing can be used discriminate direction (e.g., left vs right) \cite{mei2016listen,tellex2011understanding}.  The spatial information needed for pick-and-place tasks is substantially more precise; our findings suggest that this precision has far-reaching implications for how pointing is interpreted and how it should be modeled.



% Notes:
% Robotics: describe a cost function that you can plug into your motion generation. They point to an object and move towards it they have a very rigid cost function that exaggerates the ray...

% NLP people use a more flexible model of pointing to objects-- the pointing cone. They study cluttered scenes where the pointing cone includes several objects on the table. They use the cone basically to point to an area on the table and then they disambiguate using both natural language commands and 
% eye gaze, the shape of the arm etc.
% They don't look at the difference between objects can affect the interpretation of the pointing action. 
%%%%%%%%%%%%%%%%%

% Roboticists have indentified the importance of pointing actions in the context of conveying the intent of motions generated by robotic manipulators. Legibility was used as a metric to devise cost functions, and generate motions without speech \cite{zhao2016experimental,holladay2014legible}. These studies proposed rigid models that treats the objective of communication of the object a robot is about to pick as one of the optimization criteria. The current work studies the inherent flexibility of vague pointing gestures towards both referent objects and parts of the space, in communicating an entire pick-and-place task.



% In natural language processing, people have studied understanding and generation of multimodal referring expressions. This involves studying the generation of referring expressions together with the pointing action to pick a target referent. For instance,  \cite{kollar2014grounding,han2018placing} have shown how and when natural language is efficient for constructing spatial-semantic representations of known environments. \cite{kennington2013interpreting} proposed a discriminative model of incremental reference resolution. The focus of these studies is on designing models for generating referring expression for a target object. 
% These works do not investigate the difference between the richness of intents that pointing can express that we study in this paper. 
% This is true about cognitive and linguistics studies as well \cite{kita2003pointing,kita2003interplay,clark2003pointing,goodwin2003pointing}.
% Pointing has been the subject of cognitive and linguistics studies as a situated practice \cite{kita2003pointing,kita2003interplay,clark2003pointing,goodwin2003pointing}.
% They have relaxed the model of the pointing ray by introducing the pointing cone. 
% The pointing cone model specifies that the target of pointing action has to fall into the cone that comes out of the pointing finger \cite{rieser2004pointing,kranstedt2003deixis}. These studies do not narrow down a generative process that is necessary in the context of human robot interaction. Moreover, they do not investigate the difference between the richness of intents that pointing can express that we study in this paper.

% Our work however, proposes a new challenge. It considers the problem of a robot performing pointing actions to communicate a pick-and-place task to a human observer. This involves pointing to identify object placement in space which has been mainly overlooked by the previous literature. Furthermore, this work proposes that picking is more flexible compared to the rigid models proposed by prior work. We also study the effects of common ground and perspective on domains of referential interpretation. We present experiments testing our hypothesis. Evaluations on two different robotic arms, which are visually distinct, indicate the applicability of our hypothesis in general robotic pointing tasks.

% These works require a semantically labeled representation of the environment.













