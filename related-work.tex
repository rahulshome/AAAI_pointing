\section{Related work}
\label{related-work}

% Task communication and interpretation is an active research area in robotics and AI. 

Enabling robots to understand and generate instructions to collaboratively carry out tasks with humans is an active area of research in natural language processing and human-robot interaction \cite{butepage2017human,cha2018survey}.  Within this area, a range of work has explored the role of pointing gestures in communication.

Initial efforts in robotics have looked at making pointing gestures legible, adapting the process of motion planning so that robot movements are correctly understood as being directed toward the location of a particular object in space \cite{holladay2014legible,zhao2016experimental}.  The current work uses motions that are legible in this sense and goes on to explore how precise the targeting has to be to signal an intended interpretation.

%%%%%%%%%%%
% connections to AI and vision
%%%%%%%%%%%%
% Recent works on motion reasoning argue the importance of accurate models for gesture recognition for determining the goals and intentions of a demonstration in human--robot interaction. CITE Deieter Fox
% related problems have been studied in goal recognition CITE, goal-based imitation, and Bayesian Theory of Mind CITE. Although these works have studied the general principles of interpretable low-level motion trajectories, they fall short on studying more complicated actions such as pointing specifically to objects and locations.

% Understanding the goals and intents of actions is 

% Traditionally in robotic manipulation, tasks are specified using parameterized transformations over modeled templates of objects. Recently Manuelli et al. (2019) \cite{manuelli2019kpam} proposed a way to model the costs and constraints in robotic manipulation tasks via visual keypoints on objects which can be grounded relatively easily from language and are effective in capturing the variance in geometries within object categories for task specification. While some other work model the robotâ€™s understanding of the task via a reward-based reinforcement learning \cite{gualtieri2018pick} or via imitation learning \cite{ghasemipour2019divergence}, it is unclear how such learned representations can be used for two-way communication. Thus there is a need for interpretable representations that can be effective for a two-way task communication between robots and humans.
%%%%%%%%%%%%%%%%%%%%%

In natural language processing research, it's common to use an expanded pointing cone to describe the possible target objects for a pointing gesture, based on findings about human pointing \cite{kranstedt2003deixis,rieser2004pointing}. In cluttered scenes, the pointing cone typically includes a region with many candidate referents.  Understanding and generating object references in these situations involves combining pointing with natural language descriptions \cite{han2018placing,kollar2014grounding}.  While we also find that many pointing gestures are ambiguous and can benefit from linguistic supplementation, our results challenge the assumption of a uniform pointing cone. We argue for an alternative, context-sensitive model.

In addition to gestures that identify objects, we also look at pointing gestures that identify points in space. The closest related work involves navigation tasks, where pointing can be used to discriminate direction (e.g., left vs right) \cite{mei2016listen,tellex2011understanding}. The spatial information needed for pick-and-place tasks is substantially more precise. Our findings suggest that this precision significantly impacts how pointing is interpreted and how it should be modeled.



