\section{Design Principles}

The results of the experiments suggest that spatial pointing is interpreted rather precisely, where referential pointing is interpreted relatively flexibly.  This naturally aligns with the possibility for alternative interpretations.  For spatial reference, any location is a potential target.  However, for referential pointing, it suffices to distinguish the target object from its distractors.

We can characterize this interpretive process in formal terms by drawing on observations from the literature on vagueness \cite{kyburg2000fitting,graff2000shifting}.  Any pointing gesture starts from a set of candidate interpretations $D \subset \mathcal{W}$ determined by the context and the communicative goal.  In unconstrained situations, spatial pointing allows a full set of candidates $D = \mathcal{W}.$  If factors like common-sense physics impose task constraints, that translates to restrictions on feasible targets $CS$, leading to a more restricted set of candidates $D = CS \cap \mathcal{W}$.  Finally, for referential pointing, the potential targets are located at locations $x_1 \ldots x_N \in S$, and $D = \{ x_1 \ldots x_N \}.$

Based on the communicative setting, we know that the pointing gesture, like any vague referring expression, must select at least one of the possible interpretations \cite{kyburg2000fitting}.  We can find the best interpretation by its distance to the target $x^*$ of the pointing gesture.  Using $d(x,x^*)$ to denote this distance, gives us a threshold $$\theta = \min_{x \in D} d(x, x^*).$$

Vague descriptions can't be sensitive to fine distinctions \cite{graff2000shifting}.  So if a referent at $\theta$ is close enough to the pointing target, then another at $\theta + \epsilon$ must be close enough as well, for any value of $\epsilon$ that is not significant in the conversational context.  Our results suggest that viewers regard 10cm (in the scale of the model simulation) as an approximate threshold for a significant difference in our experiments.

In all, we predict that a pointing gesture is interpreted as referring to $\{x \in D | d(x,x^*) \leq \theta + \epsilon\}.$  We explain the different interpretations through the different choice of $D$.

\paragraph{Spatial Pointing.}  For unconstrained spatial pointing, $x^* \in D$, so $\theta=0$.  That means, the intended placement cannot differ significantly from the pointing target.  Taking into account common sense, we allow for small divergences that connect the pointing, for example, to the closest stable placement.

\paragraph{Referential Pointing.}  For referential pointing, candidates play a much stronger role.  A pointing gesture always has the closest object to the pointing target as a possible referent.  However, ambiguities arise when the geometries of more than one object intersect with the $\theta+\epsilon$-neighborhood of $x^*$.   We can think of that, intuitively, in terms of the effects of $\theta$ and $\epsilon$.  Alternative referents give rise to ambiguity not only when they are too close to the target location ($\theta$) but even when they are simply not significantly further away from the target location ($\epsilon$).  

\section{Conclusion}
\label{conclusion}

We have presented an empirical study of the interpretation of simulated robots instructing pick-and-place tasks.  Our results show that robots can effectively combine pointing gestures and spoken instructions to communicate both object information and spatial information---and offer the first empirical characterization of the use of robot gestures to communicate precise spatial locations.  The dataset and a demo of the experiments are attached with this submission, and will be released upon acceptance of the paper.

We have suggested that pointing, like other vague references, select the candidates that are 
It should be noted that the restrictions of the a simulated 2D interface of videos, and images, that serve as the form of interaction with human subjects can introduce artifacts of the effect of perspective.  
