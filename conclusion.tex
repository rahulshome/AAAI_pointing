
\section{Design Principles}

The results of the experiments suggest that spatial pointing is interpreted rather precisely, where referential pointing is interpreted relatively flexibly.  This naturally aligns with the possibility for alternative interpretations.  For spatial reference, any location is a potential target.  Nevertheless, for referential pointing, it suffices to distinguish the target object from its distractors.

We can characterize this interpretive process in formal terms by drawing on observations from the literature on vagueness \cite{kyburg2000fitting,graff2000shifting}.  Any pointing gesture starts from a set of candidate interpretations $D \subset \mathcal{W}$ determined by the context and the communicative goal.  In unconstrained situations, spatial pointing allows a full set of candidates $D = \mathcal{W}.$  If factors like common-sense physics impose task constraints, that translates to restrictions on feasible targets $CS$, leading to a more restricted set of candidates $D = CS \cap \mathcal{W}$.  Finally, for referential pointing, the potential targets are located at locations $x_1 \ldots x_N \in S$, and $D = \{ x_1 \ldots x_N \}.$

Based on the communicative setting, we know that the pointing gesture, like any vague referring expression, must select at least one of the possible interpretations \cite{kyburg2000fitting}.  We can find the best interpretation by its distance to the target $x^*$ of the pointing gesture.  Using $d(x,x^*)$ to denote this distance, gives us a threshold $$\theta = \min_{x \in D} d(x, x^*).$$

Vague descriptions can't be sensitive to fine distinctions \cite{graff2000shifting}.  So if a referent at $\theta$ is close enough to the pointing target, then another at $\theta + \epsilon$ must be close enough as well, for any value of $\epsilon$ that is not significant in the conversational context.  Our results suggest that viewers regard 10cm (in the scale of the model simulation) as an approximate threshold for a significant difference in our experiments.

In all, we predict that a pointing gesture is interpreted as referring to $\{x \in D | d(x,x^*) \leq \theta + \epsilon\}.$  We explain the different interpretations through the different choice of $D$.

\paragraph{Spatial Pointing.}  For unconstrained spatial pointing, $x^* \in D$, so $\theta=0$.  That means, the intended placement cannot differ significantly from the pointing target.  Taking into account common sense, we allow for small divergences that connect the pointing, for example, to the closest stable placement.

\paragraph{Referential Pointing.}  For referential pointing, candidates play a much stronger role.  A pointing gesture always has the closest object to the pointing target as a possible referent.  However, ambiguities arise when the geometries of more than one object intersect with the $\theta+\epsilon$-neighborhood of $x^*$.   We can think of that, intuitively, in terms of the effects of $\theta$ and $\epsilon$.  Alternative referents give rise to ambiguity not only when they are too close to the target location ($\theta$) but even when they are simply not significantly further away from the target location ($\epsilon$).  



\section{Conclusion and future work}
\label{conclusion}

We have presented an empirical study of the interpretation of simulated robots instructing pick-and-place tasks.  Our results show that robots can effectively combine pointing gestures and spoken instructions to communicate both object information and spatial information. We offer - the first, to the best of the authors' knowledge - empirical characterization of the use of robot gestures to communicate precise spatial locations for placement purposes.  The dataset and a demo of the experiments are attached with this submission, and will be released upon acceptance of the paper.  We have suggested that pointing, in line with other vague references, select from a set of candidate interpretations that depend on the task, context and communicative goal.  They pick those candidates that are not significantly further from the pointing ray than the best ones.  This contrasts with previous models that required pointing gestures to target a referent exactly or fall within a context-independent pointing cone.

Our work has a number of limitations that suggest avenues for future work.   It remains to implement the design principles on robot hardware, explore the motion planning problems associated with generating imprecise but interpretable gestures, and verify the interpretations of physically co-present viewers.  Note that we used a 2D interface, which can introduce artifacts, for example from the effect of perspective.  In addition, robots can in general trade off pointing gestures with other descriptive material in offering instructions.  Future work is needed to assess how such trade-offs play out in spatial reference, not just in object reference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%future work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Previous work by Dragan et al. and Zhao et al. have demonstrated the importance of pointing in trajectories executed by robots to refer to target objects. The current work provides a foundation to study the interpretation of different pointing actions and for designing trajectories for both referential and spatial targets. The design and evaluation of these trajectories is beyond the scope of the current work. Moreover, the addition of synchronized verbal cues is meant to draw the attention of the observer to specific states (referential and locating pointing) which must compose pick and place trajectories. Even though different types of trajectories are not evaluated, pick and place instants that compose such trajectories would be affected by the effects demonstrated in the paper.

In lab experiments

Baxter's head

