
\section{Design Principles}

The results of the experiments suggest that locating pointing is interpreted rather precisely, where referential pointing is interpreted relatively flexibly.  This naturally aligns with the possibility for alternative interpretations.  For spatial reference, any location is a potential target.  By contrast, for referential pointing, it suffices to distinguish the target object from its distractors.

We can characterize this interpretive process in formal terms by drawing on observations from the philosophical and computational literature on vagueness \cite{devault2004interpreting,graff2000shifting,kyburg2000fitting}.  Any pointing gesture starts from a set of candidate interpretations $D \subset \mathcal{W}$ determined by the context and the communicative goal.  In unconstrained situations, locating pointing allows a full set of candidates $D = \mathcal{W}.$  If factors like common-sense physics impose task constraints, that translates to restrictions on feasible targets $CS$, leading to a more restricted set of candidates $D = CS \cap \mathcal{W}$.  Finally, for referential pointing, the potential targets are located at locations $x_1 \ldots x_N \in S$, and $D = \{ x_1 \ldots x_N \}.$

Based on the communicative setting, we know that the pointing gesture, like any vague referring expression, must select at least one of the possible interpretations \cite{kyburg2000fitting}.  We can find the best interpretation by its distance to the target $x^*$ of the pointing gesture.  Using $d(x,x^*)$ to denote this distance, gives us a threshold $$\theta = \min_{x \in D} d(x, x^*).$$

Vague descriptions can't be sensitive to fine distinctions \cite{graff2000shifting}.  So if a referent at $\theta$ is close enough to the pointing target, then another at $\theta + \epsilon$ must be close enough as well, for any value of $\epsilon$ that is not significant in the conversational context.  Our results suggest that viewers regard 10cm (in the scale of the model simulation) as an approximate threshold for a significant difference in our experiments.

In all, we predict that a pointing gesture is interpreted as referring to $\{x \in D | d(x,x^*) \leq \theta + \epsilon\}.$  We explain the different interpretations through the different choice of $D$.

\paragraph{Locating Pointing}  For unconstrained locating pointing, $x^* \in D$, so $\theta=0$.  That means, the intended placement cannot differ significantly from the pointing target.  Taking into account common sense, we allow for small divergences that connect the pointing, for example, to the closest stable placement.

\paragraph{Referential Pointing}  For referential pointing, candidates play a much stronger role.  A pointing gesture always has the closest object to the pointing target as a possible referent.  However, ambiguities arise when the geometries of more than one object intersect with the $\theta+\epsilon$-neighborhood of $x^*$.   We can think of that, intuitively, in terms of the effects of $\theta$ and $\epsilon$.  Alternative referents give rise to ambiguity not only when they are too close to the target location ($\theta$) but even when they are simply not significantly further away from the target location ($\epsilon$).  



\section{Conclusion and Future Work}
\label{conclusion}

We have presented an empirical study of the interpretation of simulated robots instructing pick-and-place tasks.  Our results show that robots can effectively combine pointing gestures and spoken instructions to communicate both object information and spatial information. We offer an empirical characterization---the first, to the best of the authors' knowledge---of the use of robot gestures to communicate precise spatial locations for placement purposes.  We have suggested that pointing, in line with other vague references, select from a set of candidate interpretations that depend on the task, context and communicative goal.  They pick those candidates that are not significantly further from the pointing ray than the best ones.  This contrasts with previous models that required pointing gestures to target a referent exactly or fall within a context-independent pointing cone.

Our work has a number of limitations that suggest avenues for future work.   It remains to implement the design principles on robot hardware, explore the motion planning problems associated with generating imprecise but interpretable gestures, and verify the interpretations of physically co-present viewers.  Note that we used a 2D interface, which can introduce artifacts, for example from the effect of perspective.  In addition, robots can in general trade off pointing gestures with other descriptive material in offering instructions.  Future work is needed to assess how such trade-offs play out in location reference, not just in object reference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%future work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Previous work \cite{holladay2014legible,zhao2016experimental} has demonstrated the importance of pointing in trajectories executed by robots to refer to target objects. The current work provides a foundation to study the interpretation of different pointing actions and for designing trajectories for both referential and spatial targets. The design and evaluation of these trajectories is beyond the scope of the current work. Moreover, the addition of synchronized verbal cues is meant to draw the attention of the observer to specific states (referential and locating pointing) which must compose pick and place trajectories. Even though different types of trajectories are not evaluated, pick and place instants that compose such trajectories would be affected by the effects demonstrated in the paper.

More tight-knit collaborative scenarios need to be explored, including ones where multiple pick-and-place tasks can be composed to communicate more complex challenges and ones where they involve richer human environments.  Our study of  \textit{common sense} settings opens up intriguing avenues for such research, since it suggests ways to take into account background knowledge and expectations to narrow down the domain of possible problem specifications in composite tasks like \textit{``setting up a dining table.''} 

While the current work studies the modalities of pointing and verbal cues, effects of including additional robotic communication in the form of heads-up displays or simulated eye-gaze would be other direction to explore. Such extensions would require lab experiments with human subjects and a real robot.  This is the natural next step of our work. 

% Real world trials are invaluable in evaluating the intensity of the effects observed in the current paper, and would prove instrumental in achieving our goal of truly effective human-robot communication and collaboration.

% As an extension of the current results, lab experiments with human subjects and a real robots will be performed. Real world trials are invaluable in evaluating the intensity of the effects observed in the current paper, and would prove instrumental in achieving our goal of truly effective human-robot communication and collaboration.


\section{Acknowledgments}
The research presented here is supported by NSF
Award IIS-1526723.
Thanks to the anonymous reviewers for helpful comments. We would also like to thank the Mechanical Turk participants for their contributions.